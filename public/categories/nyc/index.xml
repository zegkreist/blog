<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nyc | Vugudum</title>
    <link>/categories/nyc/</link>
      <atom:link href="/categories/nyc/index.xml" rel="self" type="application/rss+xml" />
    <description>nyc</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>pt-br</language><lastBuildDate>Tue, 24 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>nyc</title>
      <link>/categories/nyc/</link>
    </image>
    
    <item>
      <title>Strata Data Conference Data plataform</title>
      <link>/post/strata-data-conference-data-plataform/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/strata-data-conference-data-plataform/</guid>
      <description>


&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Architecting a data plataform for enterprise use
Mark Madsen
Todd Walter&lt;/p&gt;
&lt;p&gt;more complex needs drive more complex tech&lt;/p&gt;
&lt;p&gt;complexidade trás dificuldade de inovação. Uma vez que se tem uma cadeia complicada dos dados os processos ficam dependentes deles. É necessário um grande esforço para mudar isso&lt;/p&gt;
&lt;p&gt;Grande parte dos dos projetos de inovação em TI falham. Devido há uma grande complexidade envolvida. Para resolver um problema complexo é necessário uma solução complexa.&lt;/p&gt;
&lt;p&gt;A solução para os problemas não vem pela tecnologia, mas pela arquitetura. Pensando no uso, no objetivo final do criar a plataforma de dados.
Pensar no tempo que isso irá durar. Quando o código entra em produção ele deve estar preparado para durar o tempo que for requerido para que ele funcione, a preferência é que ele dure par sempre.
Sempre pense no usuário daquela plataforma. Entáo a plataforma de dados em si depende de quais usuários estarão gerando e usando aqueles dados.
A plataforma deve gerar valor. O esforço deve ser emcima disso
Se não há um planejamento a tendência é construir coisas desnecessárias, aumentando complexidade por causa de algo que não foi se quer projetado.&lt;/p&gt;
&lt;p&gt;Vendores oferecem ferramentas e soluçoes. Eles não oferecem arquitetura. No máximo eles irão confundir e limitar a sua arquitetura&lt;/p&gt;
&lt;p&gt;A arquitetura não é estática é um processo. Não é necessariamente técnica, é ter noção de que tipos de atitudes tomar para se chegar no objetivo. Essas atitudes afetam o que já foi construído?&lt;/p&gt;
&lt;p&gt;BI
Data models depende da pergunta? Qual a pergunta?
Quando isso for construído serão feitas novas perguntas e novos modelos necessitam serem construídos
A arquitetura deve comportar isso, esse ciclo de pergunta do negócio e o sistema para responder essa pergunta.&lt;/p&gt;
&lt;p&gt;Porém há necessidades que surgem a todo tempo. O ambiente de análise de dados possui uma s;eria de ferramentas diferentes que não são substituíveis. No final será necessário manter todas essas tecnologias lado a lado. As vezes é necessário descobrir uma pergunta.&lt;/p&gt;
&lt;p&gt;As ferramentas podem ser acopladas em grupos&lt;/p&gt;
&lt;p&gt;Egines aqueles que irão utilizar os dados, R python, tableau, mnext, tensorflow, etc.&lt;/p&gt;
&lt;p&gt;A base, data stored: Dados guardados que serviram as engines de descoberta.&lt;/p&gt;
&lt;p&gt;Algo que possui os dados curados, pega do RAW e joga para as bases que fornecem para as engines de estudo&lt;/p&gt;
&lt;p&gt;RAW data&lt;/p&gt;
&lt;p&gt;Grande parte dos projetos de data science sao feitos em notebooks, não sendo necessário grandes servidores. Cuidado para não construir coisas a mais que não são necessárias.&lt;/p&gt;
&lt;p&gt;A grande questão é selecionar as technologias. Vendedores de Db querem que a solução seja seu DB, Hadoop vendors o hadoop, sparkeanos querem spark. A questão é que a construção mora em todos, não somente em um.&lt;/p&gt;
&lt;p&gt;Quando pensamos numa solução não devemos pensar só nela. Além do uso e do objetivo devemos pensar em como ela se aloca em sua vizinhança. Se construirmos um quarto precisamos entender como se ele encaixa na casa. Se construirmos uma casa precisamos entender como ela se encaixa na quadra. Como a quadra se encaixa no bairro. Como o bairro está alocado na cidade.&lt;/p&gt;
&lt;p&gt;Não podemos criar uma casa que abrigue uma cidade. O modelo de consumo de dados atual mudou, DW assumia apenas leitura, hoje nas nossas análises nos criamos dados.&lt;/p&gt;
&lt;p&gt;Não devemos pensar em ambientes uniformes. AS várias arquiteturas são apenas ferramentas, nossos ambientes deve possuir uma série de ferramentas. O embiente em que elas vivem também deve ser pensado nelas. Se há ferramentas que necessitam serem trocadas com frequencia devemos pensar numa estrutura que seja fácil trocar essas ferramentas.&lt;/p&gt;
&lt;p&gt;Devemos Zonear a arquitetura.&lt;/p&gt;
&lt;p&gt;Quando começamos a automatizar as processos geramos mais dados, logs, kpi, metricas. Com o passar do tempo acaba sendo necessário automatizar a análise dos metadados. O que acaba gerando uma nova camada de dados.&lt;/p&gt;
&lt;p&gt;Grey boxes. Combinação de modelos black box que possuim uma outra camada de modelos que avaliam suas decisões. Caso o modelo principal fique errático ou comece a tomar decisões totalmente erradas ele deve ser parado antes que faça muito estrago&lt;/p&gt;
&lt;p&gt;Isso acaba gerando mais BI. Mas há uma vantagem nisso, deixando o humano mais afastado das análises Repitivas e chatas e deixando-os mais concentrados em tarefas/ BI em decisões que necessitam uma atenção maior do humano.&lt;/p&gt;
&lt;p&gt;Devemos ter camada de automação. Verificação manual para decioes mais necessárias e Suporte de decisão (o chefe) que observa as duas outras camadas.&lt;/p&gt;
&lt;p&gt;Arquitetura. Se focarmos apenas em negócio teremos uma favela, sem infra estrutura, sem agua, sem eletricidade. Otimizado localmente mas não globalmente&lt;/p&gt;
&lt;p&gt;FAzer o design todo primeiro que os usuários seguem. Se ficar muito distante dos usuários teremos uma brasília, algo pouco funcional para o usuário.&lt;/p&gt;
&lt;p&gt;Foque nos objetivos e usuários. Constura a infraestrutura necessária para melhor atende-los&lt;/p&gt;
&lt;p&gt;1 guardar os dados raw num único local.&lt;/p&gt;
&lt;p&gt;2 Feature repository. Dados finalizados para uso de modelos, dados curados.&lt;/p&gt;
&lt;p&gt;3 Dados normalizados. Os dados são o principal elemento de reuso, não o código. Cada DW focado na aplicação de saída&lt;/p&gt;
&lt;p&gt;Os dados devem ser Tidy. Quadradinhos bonitinhos, para facilicitar a análise dos usuários. Devem ser centralizados e de fácil acesso para os analistas. Evitar silos, onde cada área possui seu banco de dados. Pois há muitas perguntas e explorações que para serem feitas necessitam do conjunto desses dados.&lt;/p&gt;
&lt;p&gt;Não é necessário curar todos os dados. Otimização muito cedo. O dado deve ser curado a medida que a necessidade de sua utilização passe a ser necessária.
Então o data lake é uma espécie de nuvem, onde os data sets que estào na beirada possuem menos necessidade de cura, enquanto os mais próximos do centro devem estar curados. Cada data set deve ter seu ‘planejamento’, no futuro esse dado deve atender tais especificações para atender tal objetivo. MVP&lt;/p&gt;
&lt;p&gt;Separe os tipos de dados. Real time Batch, incremental, one time&lt;/p&gt;
&lt;p&gt;Na terceira etapa precisamos provisionar os dados da melhor forma possível para a aplicação na frente.
[tutorialzinho] ( {{&amp;lt; relref “tutoriais/docker” &amp;gt;}})&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Strata Data Conference Securing data lakes for heavy privacy regulation</title>
      <link>/post/strata-data-conference-regulation/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/strata-data-conference-regulation/</guid>
      <description>

&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Oláá, olá a todos, espero que estejam ótimos!&lt;/p&gt;

&lt;p&gt;Trago um resumo de uma sessão que assisti no Strata Data Conference NY. O nome da sessão é: Securing data lakes for heavy privacy regulation. Foi apresentado por:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ifigeneia Derekli&lt;/li&gt;
&lt;li&gt;Mark Donsky&lt;/li&gt;
&lt;li&gt;Michael Ernest&lt;/li&gt;
&lt;li&gt;Lars George&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tudo que aqui escrevo é o meu entendimento sobre o que foi apresentado.&lt;/p&gt;

&lt;p&gt;Em todo o mundo está surgindo regulações em relação aos dados de pessoas. De fato isso é importante para proteção dos indivíduos em relação como suas informações são acessadas, utilizadas e vendidas.&lt;/p&gt;

&lt;p&gt;Estas regulações tem um forte viés na segurança, evitando que os dados sensíveis dos usuários vazem. Também na liberdade do indivíduo poder escolher se sua informação poderá ou não ser utilizada.&lt;/p&gt;

&lt;p&gt;Há formatos em que para alguém utilizar a informação da pessoa é necessário uma permissão prévia, tendo ela o poder de suspender a qualquer momento. Outros já permitem que os dados sejam utilizados sem a permissão prévia, mas guardando o direito do usuário suspender as atividades.&lt;/p&gt;

&lt;p&gt;Estas regulações virão cada vez mais, com diversas abrangências e diversos requisitos. Mas se tomarmos boas práticas de agora podemos deixar a estrutura pronta para se adequar a essas novas normas. Basicamente devemos assegurar que nossos serviços tenham:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;segurança: controle de acesso, criptografia, autenticação&lt;/li&gt;
&lt;li&gt;whitelist: poder filtrar os dados somente para aquelas pessoas que deram permissão de uso&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;São coisas simples, mas que podem ser quebradas nos quatro tópicos a seguir:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Autenticação&lt;/li&gt;
&lt;li&gt;Autorização&lt;/li&gt;
&lt;li&gt;Criptografia&lt;/li&gt;
&lt;li&gt;Regulação&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;autenticação&#34;&gt;Autenticação&lt;/h2&gt;

&lt;p&gt;A ideia aqui é descobrir quem você é! O Usuário precisar provar sua identidade para o sistema.
Em tempos de home office essa parte é muito importante, pois é uma porta de entrada. Esta porta deve ser aberta somente para pessoas autorizadas.&lt;/p&gt;

&lt;p&gt;As pessoas autorizadas estão numa &amp;ldquo;lista&amp;rdquo; seleta, o usuário precisa então provar sua identidade. A identidade é um objeto único identificatório, um username, um e-mail, etc. Mas uma identidade não significa conta, uma conta pode possuir mais de uma identidade.&lt;/p&gt;

&lt;p&gt;Agora a prova fornecida para atestar a veracidade da pessoa em relação a identidade pode ser de três tipos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Coisas que você sabe: uma senha.&lt;/li&gt;
&lt;li&gt;Coisas que você possui: um smart card, um app de tokens, etc.&lt;/li&gt;
&lt;li&gt;Coisas que você é: sua Iris, digitais, DNA, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quanto mais dessas acumuladas melhor para a segurança. Por isso sempre use MFA.&lt;/p&gt;

&lt;p&gt;Está etapa é extremamente sensível, cheia de pormenores, é recomendado que não se faça uma aplicação para isso, mas sim utilizar algo já pronto e testado, um software ou serviço. Há vários serviços e programas que fazem essa parte, o importe é utilizar algo que se encaixe no que você precisa. Que seja um serviço que tenha uma equipe dando suporte, corrigindo bugs e fazendo melhorias constantemente.&lt;/p&gt;

&lt;p&gt;Há gerenciadores em filesystem, como o login de linux ou até mesmo no Windows. Mas isso não foi feito para ser utilizado em larga escala. Existem métodos de onde a autenticação da identidade é feita por um agente externo como LDAP. Hoje, também temos a possibilidade de sistema de autenticação como serviço, a exemplo do AWS cognito, Azure active directory, Auth0, etc.  Eles podem usar diferentes tipos de tecnologia por trás, OAuth, OpenID,  SAML. Esses caras como serviço facilitam o transito das identidades entre os recursos de uma forma fácil e segura, onde tokens são gerados na hora, com curto tempo de validade, etc. Sem decorar a porrada de senhas. É fácil identificar o acesso de algo e cortar os acessos como um todo.&lt;/p&gt;

&lt;h2 id=&#34;autorização&#34;&gt;Autorização&lt;/h2&gt;

&lt;p&gt;Uma vez que o sistema sabe quem eu sou a pergunta aqui agora muda. Se eu sou quem eu sou, o que eu posso fazer?&lt;/p&gt;

&lt;p&gt;O objetivo dessa parte é colocar cada um no seu quadrado. Há diferentes tipos de usuários. Com isso há diferentes tipos de permissões para cada usuário. Tem aqueles que podem tomar ações, enquanto outros devem apenas consumir algo, também temos o que fazem os dois. Em cima disso tudo ainda há uma questão de zonas. Em quais dados / zonas os usuários poderão consumir/ tomar ações. É necessário manter controle de todo esse  &amp;ldquo;movimento&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Existem diversas formas de realizar esse controle, os palestrantes citaram os seguintes exemplos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ACL (Access control lists): ACL é usado diretamente no objeto. S3, HDFS, Hbase, ADLS Gen2, GCs, etc. PROS: simples, da conta do recado, bem comum no mercado. CONS: baixo nível. Não pode ser aplicado holisticamente, tem que ser manejado sistema por sistema, arquivo a arquivo.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RBAC (Role based access control): Role é uma função que tem significado para uma empresa ou grupo. Usuários ou grupos recebem uma ou mais funções. Cada Função é associada com certas permissões (políticas). As funções são extraídas da identidade, feitas depois da autenticação, permitindo que elas sejam manejadas separadamente. Exemplo: AWS IAM e google IAM. POS: simples, fácil entendimento, mais eficiência, compliance. CONS: não é dinâmico o suficiente, não é muito granular (pode ser feito, mas exige mais esforço), no final pode haver muitas funções para manejar.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ABAC (Attribute based access control): Atributo é algo que descreve o usuário/grupo/dados/contexto. É Mais dinâmico que o RBAC, se o atributo muda as permissões o segue. POS: dinâmico, granularidade baixa, eficiência, compliance. CONS: bem mais complexo que RBAC, pode continuar com muitas políticas.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O importante é poder ter ao mesmo tempo granularidade nas permissões e controle sobre as permissões. Isto para que se tenha um sistema flexível e seguro. Caso tenha uma falha nos acessos que seja possível identificá-la de forma rápida e resolvê-la sem grandes efeitos colaterais.&lt;/p&gt;

&lt;p&gt;As políticas podem cuidar disso, mas as permissões de visualização deve descer ao nível da linha do dado. Imagine que, uma pessoa permita que você possa utilizar seus dados, mas não sua filial na Europa. Então as visualizações &amp;ldquo;europeias&amp;rdquo; não devem receber esses dados em específico.&lt;/p&gt;

&lt;p&gt;Numa arquitetura de banco de dados, é uma boa prática nunca exibir as tabelas bases para ninguém. O usuário/analista/cientista de dados/executivo que for acessar estes dados devem o fazer via uma camada que verifica seus acessos. Assim a pessoa que irá trabalhar com estes dados irá ver uma versão da tabela base filtrada por uma tabela de controle de acesso.&lt;/p&gt;

&lt;h2 id=&#34;criptografia&#34;&gt;Criptografia&lt;/h2&gt;

&lt;p&gt;Confesso que este foi o ponto da palestra em que menos me sinto confortável para falar. Por isso irei apenas passar por cima, num modo mais conceitual.&lt;/p&gt;

&lt;p&gt;A pergunta aqui não é &amp;ldquo;Devemos criptografar os dados?&amp;rdquo;, mas sim &amp;ldquo;Quando criptografar os dados?&amp;rdquo;. A criptografia oferece proteção e confidencialidade aos nossos dados, dados criptografados mesmo que vazados estão seguros (desde que capturou não tenha a chave de decodificação). É necessário manter integridade dos dados (evitando perdas acidentais) e a confidencialidade. Com a técnica apropriada é possível manter os dois.&lt;/p&gt;

&lt;p&gt;Quando transferimos dados de um lado para outro nós estamos utilizando fios, literalmente. Caso estes fios estejam numa rede pública, na internet por exemplo, eles são acessíveis para outras pessoas. Caso não seja utilizado algum tipo de criptografia em trânsito os dados estão sendo transferidos abertamente. Qualquer um com acesso nesta rede pública pode capturar estes dados sendo transferidos e observar o que eles realmente são. Agora, utilizando algum tipo de criptografia de transferência os dados ainda podem ser &amp;ldquo;capturados&amp;rdquo;, porém não são recuperáveis, com exceção das pontas (aquele que enviou o dados e aquele que deve receber). Basicamente o que é capturado é uma colação de bytes aleatórios.&lt;/p&gt;

&lt;p&gt;Quando os dados estão parados (at rest) também é importante criptografar os dados. Pois isso protege os dados de pessoas que já estão na rede, pode ser um usuário interno mal intencionado, ou até mesmo uma invasão.&lt;/p&gt;

&lt;p&gt;Porém tudo isso vem a um custo. Para podermos manter os dados criptografados é necessário que seja feito um manejamento das chaves de criptografia. São por estas que é possível criptografar e descriptografar os dados. E isto é de extrema importância, veja, se alguém tiver acesso às chaves ele terá acesso aos dados. Caso alguém se atrapalhe e as chaves sejam perdidas você nunca mais terá acesso a aqueles dados. Então é necessário que tenha um processo de governança sobre estas chaves, deixando elas num local diferentes dos dados, com expiração e rotação, com backups, etc.&lt;/p&gt;

&lt;h2 id=&#34;regulação&#34;&gt;Regulação&lt;/h2&gt;

&lt;p&gt;Não é possível prever todos os detalhes de um nova regulação. Porém com boas práticas podemos estar preparados para elas. Fornecendo segurança, controle de acesso linha a linha para usuários são exemplos disso.&lt;/p&gt;

&lt;p&gt;E necessário conhecer conhecer quais dados você possui no seu data lake. Quais são os dados sensíveis, quais conjunto de dados, mesmo que não sejam sensíveis possam identificar um indivíduo. Crie catálogos de todos os dados, crie tags para cada coluna, metadados EVERYWHERE.&lt;/p&gt;

&lt;p&gt;Como os dados estão sendo usados? Por quem os dados estão sendo usados.&lt;/p&gt;

&lt;p&gt;Saber a linha temporal dos dados. De quando ele entra até quando ele sai para o usuário final, quais são as etapas de transformações. Guarde o estado de cada cálculo, entrada e saída, hoje o custo de armazenamento de dados é o mais baixo da história. Podemos guardar todo o histórico de transformação. Está é uma etapa necessária, já que muitas das regulações que estão surgindo ao redor do mundo exigem que isto seja feito e por um longo período de tempo.&lt;/p&gt;

&lt;p&gt;SEMPRE devemos pensar na segurança por default. Hoje, muitos sistema são construídos para então serem adicionados etapas de segurança. Isto deve ser invertido, devemos pensar nos sistemas e nas soluções de forma a fornecer o máximo de segurança. Encriptar dados, controle de acesso, anonimização pseudoanonimização (xuberar), acesso fino aos dados.&lt;/p&gt;

&lt;p&gt;Sempre implementar mecanismos de validar o consentimento das pessoas que de fato são os donos de suas informações pessoais. Toda aplicação deve receber os dados após a validação do consentimento, nunca dos dados base.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
